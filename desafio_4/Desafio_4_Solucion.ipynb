{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bbbc73",
   "metadata": {},
   "source": [
    "# Desafío 4 - Bot QA con Seq2Seq\n",
    "\n",
    "## Paola Cartalá"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8477d",
   "metadata": {},
   "source": [
    "## Librerías e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70735259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264fcd7",
   "metadata": {},
   "source": [
    "## Configuración del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 8000\n",
    "MAX_LENGTH = 15\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_UNITS = 128\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "DATA_SAMPLES = 40000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb32cf1",
   "metadata": {},
   "source": [
    "## Dataset ConvAI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb33dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'data_volunteers.json'\n",
    "if not os.path.exists(dataset_file):\n",
    "    print(\"Descargando el dataset ConvAI2...\")\n",
    "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
    "    gdown.download(url, dataset_file, quiet=False)\n",
    "else:\n",
    "    print(\"El dataset ya se encuentra descargado.\")\n",
    "\n",
    "with open(dataset_file) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475ff8f",
   "metadata": {},
   "source": [
    "## Preprocesamiento y Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Limpia y normaliza texto expandiendo contracciones y removiendo caracteres especiales.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = [], []\n",
    "for conversation in data:\n",
    "    for i in range(len(conversation['dialog']) - 1):\n",
    "        q = clean_text(conversation['dialog'][i]['text'])\n",
    "        a = clean_text(conversation['dialog'][i+1]['text'])\n",
    "        if len(q.split()) < MAX_LENGTH and len(a.split()) < MAX_LENGTH:\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "\n",
    "answers_input = ['<sos> ' + a for a in answers]\n",
    "answers_target = [a + ' <eos>' for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenización\n",
    "tokenizer_q = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<unk>')\n",
    "tokenizer_q.fit_on_texts(questions)\n",
    "encoder_sequences = tokenizer_q.texts_to_sequences(questions)\n",
    "\n",
    "tokenizer_a = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<unk>', filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer_a.fit_on_texts(answers_input + answers_target)\n",
    "decoder_input_sequences = tokenizer_a.texts_to_sequences(answers_input)\n",
    "decoder_target_sequences = tokenizer_a.texts_to_sequences(answers_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "encoder_input = pad_sequences(encoder_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "vocab_size_q = len(tokenizer_q.word_index) + 1\n",
    "vocab_size_a = len(tokenizer_a.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e5f484",
   "metadata": {},
   "source": [
    "## Modelo Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_modelo_qa_bot(vocab_size_q, vocab_size_a, embedding_dim, hidden_units):\n",
    "    \"\"\"Crea modelo Seq2Seq con encoder bidireccional y decoder LSTM para QA Bot.\"\"\"\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    encoder_embedding = Embedding(vocab_size_q, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(\n",
    "        LSTM(hidden_units, return_state=True, dropout=0.2)\n",
    "    )(encoder_embedding)\n",
    "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_embedding_layer = Embedding(vocab_size_a, embedding_dim, mask_zero=True)\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "    decoder_lstm = LSTM(hidden_units * 2, return_sequences=True, return_state=True, dropout=0.2)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(vocab_size_a, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "modelo = crear_modelo_qa_bot(vocab_size_q, vocab_size_a, EMBEDDING_DIM, HIDDEN_UNITS)\n",
    "modelo.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ddfe0",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f83107",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_train, encoder_val, dec_in_train, dec_in_val, dec_target_train, dec_target_val = train_test_split(\n",
    "    encoder_input, decoder_input, decoder_target, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = modelo.fit(\n",
    "    [encoder_train, dec_in_train], dec_target_train,\n",
    "    validation_data=([encoder_val, dec_in_val], dec_target_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb043817",
   "metadata": {},
   "source": [
    "## Sistema de Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QABotInference:\n",
    "    \"\"\"Sistema de inferencia para QA Bot usando temperatura y top-k.\"\"\"\n",
    "    def __init__(self, model, tokenizer_q, tokenizer_a, max_length):\n",
    "        \"\"\"Inicializa el sistema de inferencia con modelo y tokenizers.\"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer_q = tokenizer_q\n",
    "        self.tokenizer_a = tokenizer_a\n",
    "        self.max_length = max_length\n",
    "        self.idx_to_word_a = {v: k for k, v in tokenizer_a.word_index.items()}\n",
    "        self.sos_token_id = tokenizer_a.word_index.get('<sos>')\n",
    "        self.eos_token_id = tokenizer_a.word_index.get('<eos>')\n",
    "\n",
    "    def generar_respuesta(self, pregunta, temperature=0.7, top_k=10):\n",
    "        \"\"\"Genera respuesta usando muestreo con temperatura y top-k.\"\"\"\n",
    "        pregunta_limpia = clean_text(pregunta)\n",
    "        input_seq = self.tokenizer_q.texts_to_sequences([pregunta_limpia])\n",
    "        input_padded = pad_sequences(input_seq, maxlen=self.max_length, padding='post')\n",
    "\n",
    "        decoder_input_seq = np.zeros((1, self.max_length))\n",
    "        decoder_input_seq[0, 0] = self.sos_token_id\n",
    "        respuesta_generada = []\n",
    "\n",
    "        for i in range(1, self.max_length):\n",
    "            output_tokens = self.model.predict([input_padded, decoder_input_seq], verbose=0)\n",
    "            probs = output_tokens[0, i-1, :]\n",
    "\n",
    "            probs = np.asarray(probs).astype('float64')\n",
    "            probs = np.log(probs) / temperature\n",
    "            exp_probs = np.exp(probs)\n",
    "            probs = exp_probs / np.sum(exp_probs)\n",
    "            \n",
    "            top_indices = np.argsort(probs)[-top_k:]\n",
    "            top_probs = probs[top_indices]\n",
    "            top_probs = top_probs / np.sum(top_probs)\n",
    "\n",
    "            predicted_token_id = np.random.choice(top_indices, p=top_probs)\n",
    "            \n",
    "            if predicted_token_id == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "            word = self.idx_to_word_a.get(predicted_token_id, '')\n",
    "            respuesta_generada.append(word)\n",
    "            decoder_input_seq[0, i] = predicted_token_id\n",
    "\n",
    "        return ' '.join(respuesta_generada).capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187cff4",
   "metadata": {},
   "source": [
    "## Evaluación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = QABotInference(modelo, tokenizer_q, tokenizer_a, MAX_LENGTH)\n",
    "\n",
    "print(\"🤖 EVALUACIÓN FINAL DEL BOT\")\n",
    "\n",
    "preguntas_evaluacion = [\n",
    "    \"Do you read?\",\n",
    "    \"Do you have any pet?\",\n",
    "    \"Where are you from?\",\n",
    "\n",
    "    \"How are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"what are your hobbies?\"\n",
    "]\n",
    "\n",
    "for pregunta in preguntas_evaluacion:\n",
    "    respuesta = bot.generar_respuesta(pregunta)\n",
    "    print(f\"👤 USER: {pregunta}\")\n",
    "    print(f\"🤖 BOT: {respuesta}\\n\" + \"-\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
